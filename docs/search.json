[
  {
    "objectID": "research_academic_work.html",
    "href": "research_academic_work.html",
    "title": "Research",
    "section": "",
    "text": "Istanbul Technical University, 2025\nSupervisor: Prof. Dr. Osman Doğan\n\n\n\n\n\n\nAbstract (EN)\n\n\n\n\n\nIn this thesis, we examine the impact of curative and preventive health expenditures on economic performance across Organisation for Economic Co-operation and Development (OECD) countries from 1998 to 2021. The existing literature generally agrees that health spending has a positive effect on Gross Domestic Product (GDP) growth, in line with human capital theory. However, fewer studies make a distinction between types of health investment, such as curative (treating illnesses) and preventive (avoiding illnesses). We believe that optimizing these expenditures is crucial in situations involving policy trade-offs. The aim of this thesis is to estimate the causal effect of curative and preventive spending, and to explore interactions with education investment and differentiation by income level. Additionally, we aim to apply an innovative method, such as Double Machine Learning (DML), in an area where it has not been frequently used before.\nWe obtained a balanced panel dataset of 29 OECD countries over the period 1998-2021 from OECD Health Statistics and the World Bank. The methodology employs two main approaches: traditional Fixed Effects (FE) models with year and country dummy variables, and a DML framework with high-dimensional predictors, utilizing five machine learning methods (Lasso, Ridge, Elastic Net, Random Forest, and Extreme Gradient Boosting (XGBoost)). Our primary variables of interest are curative and preventive health expenditures as a percentage of GDP. Controls include demographic variables (e.g., life expectancy, elderly population) and economic variables (e.g., savings, trade openness, short-term interest rate).\nFE models reveal that curative expenditure may show a nonlinear, U-shaped relationship with the income level. The turning point is 2.87% of GDP after passing that point, curative health spending positively contributes to economic development. This finding is consistent across contemporaneous and lagged FE models. Regarding preventive expenditure, we were unable to identify a significant impact on the logarithm of GDP per capita. Additionally, regressions based on income level showed that the positive effects of curative spending become significant only for higher-income OECD countries, but not for lower-income ones. We also found that higher education spending moderates the effect of curative health expenditure in promoting economic development.\nThe DML estimates provide interesting results as well. For curative spending, DML models agree on a convex relationship. Contrary to FE models, some DML models identified an inverted U-shaped relationship between preventive health expenditure and the dependent variable; however, for lagged preventive spending models disagree.\nWe can derive important policy implications from these results. Firstly, curative health investment can increase the income level, but only if the spending exceeds the minimum threshold, and investment in education is also necessary. Secondly, we suggest that countries should first invest in education and then expand curative healthcare spending. Thirdly, countries need to improve the tracking and classification of preventive expenditures to assess their effect on economic performance with certainty. Our DML results suggest that returns to prevention may be underestimated due to current data limitations. For high-income countries, policymakers should consider a higher threshold for the curative spending for it to contribute to economic development. To sum up, this thesis demonastrates that we need to explore novel approaches to drive discoveries in health economics.\n\n\n\n\n\nView Presentation Slides (PDF)\n\n\n\n\nFull Thesis\nPresentation\n\n\n\n\n\n\n\nKey Highlights\n\n\n\n\n\n\nDataset: 29 OECD countries (1998–2021) using OECD Statistics & World Bank data\n\nMethods:\n\nFixed Effects (FE) models with lags and dummies\n\nDouble Machine Learning (Lasso, Ridge, Elastic Net, RF, XGBoost)\n\nMain findings:\n\nCurative spending shows U-shaped impact on GDP with a turning point at 2.87% of GDP\nPreventive spending not significant in FE models; DML models show mixed patterns\nCurative effects are income-sensitive\nEducation spending boosts curative effectiveness\n\nPolicy implications: context matters:\n\nPositive economic effects appear only at a later threshold for high-income or high-education settings\nHealth–education policy alignment\nEfficiency benchmarks for health spending (GDP thresholds)\nImproved classification/tracking of preventive spending"
  },
  {
    "objectID": "research_academic_work.html#masters-thesis",
    "href": "research_academic_work.html#masters-thesis",
    "title": "Research",
    "section": "",
    "text": "Istanbul Technical University, 2025\nSupervisor: Prof. Dr. Osman Doğan\n\n\n\n\n\n\nAbstract (EN)\n\n\n\n\n\nIn this thesis, we examine the impact of curative and preventive health expenditures on economic performance across Organisation for Economic Co-operation and Development (OECD) countries from 1998 to 2021. The existing literature generally agrees that health spending has a positive effect on Gross Domestic Product (GDP) growth, in line with human capital theory. However, fewer studies make a distinction between types of health investment, such as curative (treating illnesses) and preventive (avoiding illnesses). We believe that optimizing these expenditures is crucial in situations involving policy trade-offs. The aim of this thesis is to estimate the causal effect of curative and preventive spending, and to explore interactions with education investment and differentiation by income level. Additionally, we aim to apply an innovative method, such as Double Machine Learning (DML), in an area where it has not been frequently used before.\nWe obtained a balanced panel dataset of 29 OECD countries over the period 1998-2021 from OECD Health Statistics and the World Bank. The methodology employs two main approaches: traditional Fixed Effects (FE) models with year and country dummy variables, and a DML framework with high-dimensional predictors, utilizing five machine learning methods (Lasso, Ridge, Elastic Net, Random Forest, and Extreme Gradient Boosting (XGBoost)). Our primary variables of interest are curative and preventive health expenditures as a percentage of GDP. Controls include demographic variables (e.g., life expectancy, elderly population) and economic variables (e.g., savings, trade openness, short-term interest rate).\nFE models reveal that curative expenditure may show a nonlinear, U-shaped relationship with the income level. The turning point is 2.87% of GDP after passing that point, curative health spending positively contributes to economic development. This finding is consistent across contemporaneous and lagged FE models. Regarding preventive expenditure, we were unable to identify a significant impact on the logarithm of GDP per capita. Additionally, regressions based on income level showed that the positive effects of curative spending become significant only for higher-income OECD countries, but not for lower-income ones. We also found that higher education spending moderates the effect of curative health expenditure in promoting economic development.\nThe DML estimates provide interesting results as well. For curative spending, DML models agree on a convex relationship. Contrary to FE models, some DML models identified an inverted U-shaped relationship between preventive health expenditure and the dependent variable; however, for lagged preventive spending models disagree.\nWe can derive important policy implications from these results. Firstly, curative health investment can increase the income level, but only if the spending exceeds the minimum threshold, and investment in education is also necessary. Secondly, we suggest that countries should first invest in education and then expand curative healthcare spending. Thirdly, countries need to improve the tracking and classification of preventive expenditures to assess their effect on economic performance with certainty. Our DML results suggest that returns to prevention may be underestimated due to current data limitations. For high-income countries, policymakers should consider a higher threshold for the curative spending for it to contribute to economic development. To sum up, this thesis demonastrates that we need to explore novel approaches to drive discoveries in health economics.\n\n\n\n\n\nView Presentation Slides (PDF)\n\n\n\n\nFull Thesis\nPresentation\n\n\n\n\n\n\n\nKey Highlights\n\n\n\n\n\n\nDataset: 29 OECD countries (1998–2021) using OECD Statistics & World Bank data\n\nMethods:\n\nFixed Effects (FE) models with lags and dummies\n\nDouble Machine Learning (Lasso, Ridge, Elastic Net, RF, XGBoost)\n\nMain findings:\n\nCurative spending shows U-shaped impact on GDP with a turning point at 2.87% of GDP\nPreventive spending not significant in FE models; DML models show mixed patterns\nCurative effects are income-sensitive\nEducation spending boosts curative effectiveness\n\nPolicy implications: context matters:\n\nPositive economic effects appear only at a later threshold for high-income or high-education settings\nHealth–education policy alignment\nEfficiency benchmarks for health spending (GDP thresholds)\nImproved classification/tracking of preventive spending"
  },
  {
    "objectID": "research_academic_work.html#bachelors-thesis",
    "href": "research_academic_work.html#bachelors-thesis",
    "title": "Research",
    "section": "Bachelor’s Thesis",
    "text": "Bachelor’s Thesis\n\nImpact of COVID-19 on the Job Satisfaction of Russian Employees\nNational Research University Higher School of Economics, 2023\nSupervisor: E.A. Zazdravnykh, PhD (Economics)\n\n\n\n\n\n\nAbstract (EN)\n\n\n\n\n\nThis study aimed to estimate the effect of COVID-19 pandemic on job satisfaction across low, moderate, and high COVID-19 risk groups within the Russian workforce, focusing on satisfaction with job in general, working conditions, growth opportunities, and pay. This study utilized Heckman probit model using secondary data obtained from the “Russia Longitudinal Monitoring Survey, RLMS-HSE” for the years 2019-2020, collected by annual nationwide representative survey using probability stratified multistage area sampling. We found that not all aspects of job satisfaction were impacted by COVDI-19. Results showed a decline in satisfaction with working conditions among high COVID-19 risk workers, while satisfaction in this aspect increased in the moderate risk group. Low-risk workers displayed higher job satisfaction in general, with growth opportunities, while moderate-risk workers showed higher satisfaction with pay. These findings suggest that the pandemic has exacerbated disparities in job satisfaction, with the most adverse effects being felt by high-risk and vulnerable workers. This study contributes to the literature by examining the differential impact of the COVID-19 pandemic on job satisfaction across various risk groups in the Russian context and interpreting the results based on Job Demands-Resources Model. Limitations include a relatively small sample size for the high-risk group and lack of extensive data on vulnerable professions. Future research could expand the sample size, incorporate more years of data, and explore monthly trends to provide a more comprehensive understanding of the effect of COVID-19 on job satisfaction. Keywords: job satisfaction, employee well-being, COVID-19, JD-R model, Russian workforce\n\n\n\n\nMy Thesis Presentation\nView Presentation Slides (PDF)\n\n\nDownloads\n\nFull Thesis\nPresentation\n\n\n\n\n\n\n\nKey Highlights\n\n\n\n\n\n\nLow and Moderate Risk Groups\n\nSignificant increase in general job satisfaction in 2020.\nThis aligns with previous studies (Frutos-Bencze et al., 2022)._\nMore likely to express satisfaction with growth opportunities in 2020.\nSupported by previous studies (Nicola et al., 2020) and the JD-R model._\n\n\n\nModerate Risk Group\n\nMore likely to be satisfied with pay in 2020.\nSignificant increase in satisfaction with working conditions in 2020.\n\n\n\nHigh Risk Group\n\nNo significant decrease in overall job satisfaction.\nThis contradicts previous research, but may be explained by psychological resilience (Caponnetto et al., 2022) or by limitations of the current study._\nLower likelihood of satisfaction with working conditions in 2020.\nThis finding aligns with earlier research (Chirico et al., 2020)._\n\n\n\nTheoretical Contribution\n\nAddresses the research gap by analyzing the COVID-19 impact on the Russian workforce, going beyond remote workers and developed economies.\nUtilizes the Heckman probit model to account for selection bias.\nConsiders multiple dimensions of job satisfaction, providing a more comprehensive understanding of the pandemic’s effects.\n\n\n\nPractical Significance (Managerial Application)\n\nHighlights the need for tailored support strategies for different risk groups in times of crisis.\nEspecially relevant for high-risk professions._\nShows that low job satisfaction may result in higher turnover, particularly harmful for essential workers.\nOffers actionable insights for policy design:\n\nImprove working conditions for high-risk groups.\nImplement flexible work arrangements.\nEnsure access to protective equipment."
  },
  {
    "objectID": "research_academic_work.html#theoretical-contribution",
    "href": "research_academic_work.html#theoretical-contribution",
    "title": "Research",
    "section": "Theoretical Contribution",
    "text": "Theoretical Contribution\n\nAddresses the research gap by analyzing the COVID-19 impact on the Russian workforce, going beyond remote workers and developed economies.\nUtilizes the Heckman probit model to account for selection bias.\nConsiders multiple dimensions of job satisfaction, providing a more comprehensive understanding of the pandemic’s effects."
  },
  {
    "objectID": "research_academic_work.html#practical-significance-managerial-application",
    "href": "research_academic_work.html#practical-significance-managerial-application",
    "title": "Research",
    "section": "Practical Significance (Managerial Application)",
    "text": "Practical Significance (Managerial Application)\n\nHighlights the need for tailored support strategies for different risk groups in times of crisis.\nEspecially relevant for high-risk professions._\nShows that low job satisfaction may result in higher turnover, particularly harmful for essential workers.\nOffers actionable insights for policy design:\n\nImprove working conditions for high-risk groups.\nImplement flexible work arrangements.\nEnsure access to protective equipment."
  },
  {
    "objectID": "research_academic_work.html#want-to-know-more",
    "href": "research_academic_work.html#want-to-know-more",
    "title": "Research",
    "section": "Want to Know More?",
    "text": "Want to Know More?\nIf you’re interested in the data, code, or collaboration around this research, feel free to contact me or view my portfolio."
  },
  {
    "objectID": "projects/kickstarter.html",
    "href": "projects/kickstarter.html",
    "title": "Kickstarter Project Data Analysis",
    "section": "",
    "text": "The goal of the project is to investigate the factors influencing the success of projects hosted on the Kickstarter platform. The focus is on categorical variables, launch time, and project name characteristics. A classification decision tree model was constructed using the rpart() function, which predicts project success.\nView full report with graphs and analysis\nView dashboard\nView code"
  },
  {
    "objectID": "projects/kickstarter.html#most-expensive-successful-projects-by-category",
    "href": "projects/kickstarter.html#most-expensive-successful-projects-by-category",
    "title": "Kickstarter Project Data Analysis",
    "section": "3.1 Most Expensive Successful Projects by Category",
    "text": "3.1 Most Expensive Successful Projects by Category\nThe highest average revenue was collected in the following categories: - Technology - Design - Food"
  },
  {
    "objectID": "projects/kickstarter.html#percentage-of-successful-projects-by-category",
    "href": "projects/kickstarter.html#percentage-of-successful-projects-by-category",
    "title": "Kickstarter Project Data Analysis",
    "section": "3.2 Percentage of Successful Projects by Category",
    "text": "3.2 Percentage of Successful Projects by Category\nMost Successful Categories: - Comics - Dance - Theater - Games - Music\nInterestingly, the most expensive categories (e.g., Technology) show the lowest success rate (~25%)."
  },
  {
    "objectID": "projects/kickstarter.html#subcategories-gen_category",
    "href": "projects/kickstarter.html#subcategories-gen_category",
    "title": "Kickstarter Project Data Analysis",
    "section": "3.3 Subcategories (gen_category)",
    "text": "3.3 Subcategories (gen_category)\nAnalysis by subcategory revealed: - Higher revenue in specific subcategories (e.g., Movie Theaters, Camera Equipment). - A more precise understanding of success—for example, Video Games lower the average success in the Games category, while Tabletop Games increase it."
  },
  {
    "objectID": "projects/kickstarter.html#statistical-tests",
    "href": "projects/kickstarter.html#statistical-tests",
    "title": "Kickstarter Project Data Analysis",
    "section": "3.4 Statistical Tests",
    "text": "3.4 Statistical Tests\nA test of independence was conducted between success and: - category - main_category - gen_category\nIn all cases, p-value &lt; 0.05, the hypothesis of significant differences is confirmed."
  },
  {
    "objectID": "projects/kickstarter.html#complex-model-rpart-without-feature-selection",
    "href": "projects/kickstarter.html#complex-model-rpart-without-feature-selection",
    "title": "Kickstarter Project Data Analysis",
    "section": "6.1 Complex Model (rpart, without feature selection)",
    "text": "6.1 Complex Model (rpart, without feature selection)\n\nAccuracy on training set: 0.70\nAccuracy on test set: 0.68\nTree pruning based on optimal cp was applied – accuracy remained virtually unchanged.\n\nThe most important variables are: - category, gen_category, main_category - cat_first_letter, gen_cat_first_letter - usd_goal_real, period_length, name_length - launch_dayPart, currency, country, launch_hour"
  },
  {
    "objectID": "projects/kickstarter.html#simplified-model",
    "href": "projects/kickstarter.html#simplified-model",
    "title": "Kickstarter Project Data Analysis",
    "section": "6.2 Simplified Model",
    "text": "6.2 Simplified Model\nA model with a smaller number of features was built based on the variable importance from Model 1:\n\nAccuracy on the training set: 0.675\nAccuracy on the testing set: 0.669\n\nThe decrease in accuracy is minor, but the model is simpler and easier to interpret."
  },
  {
    "objectID": "projects/kickstarter.html#interpretation-of-groups",
    "href": "projects/kickstarter.html#interpretation-of-groups",
    "title": "Kickstarter Project Data Analysis",
    "section": "6.3 Interpretation of Groups",
    "text": "6.3 Interpretation of Groups\nThe simplified model identifies seven groups of projects with different probabilities of success:\n\nUnfavorable subcategories → unsuccessful\nFavorable subcategories → successful\nHigh goal (&gt;$31,000) → unsuccessful\nLong campaign (&gt;56 days) → unsuccessful\nLow goal (&lt;$1,418) → successful\nEvening/night launch → unsuccessful\nDaytime launch + moderate goal → successful"
  },
  {
    "objectID": "projects/employee-churn.html",
    "href": "projects/employee-churn.html",
    "title": "R&D Employee Churn Analysis",
    "section": "",
    "text": "The project included an analysis of employee attrition factors in the strategically important Research & Development department. Data from an SQL database was used, combining the profile, portfolio, and education tables. A logistic regression model was built based on the processed and balanced data.\nOpen interactive dashboard\nView full report with code, graphs, and analysis"
  },
  {
    "objectID": "projects/employee-churn.html#project-summary",
    "href": "projects/employee-churn.html#project-summary",
    "title": "R&D Employee Churn Analysis",
    "section": "",
    "text": "The project included an analysis of employee attrition factors in the strategically important Research & Development department. Data from an SQL database was used, combining the profile, portfolio, and education tables. A logistic regression model was built based on the processed and balanced data.\nOpen interactive dashboard\nView full report with code, graphs, and analysis"
  },
  {
    "objectID": "projects/employee-churn.html#methods-and-features",
    "href": "projects/employee-churn.html#methods-and-features",
    "title": "R&D Employee Churn Analysis",
    "section": "Methods and Features",
    "text": "Methods and Features\n\nSQL queries to the database + preprocessing in R\nVisualization of distributions by gender, age, position, and salary\nLogistic regression with SMOTE for class balancing\nModel evaluation: accuracy 0.81, sensitivity 0.82\nModel interpretation using Odds Ratio and the influence of factors\nSimulations: the impact of reducing overtime and increasing satisfaction"
  },
  {
    "objectID": "projects/employee-churn.html#conclusions",
    "href": "projects/employee-churn.html#conclusions",
    "title": "R&D Employee Churn Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nThe main factors increasing the risk of leaving: - overtime (OverTime = Yes) - frequent Business trips - dissatisfaction with work conditions and relationships - low engagement\nFactors that reduce the risk of attrition: - high engagement - high job satisfaction - doctoral degree\nSpecific measures were proposed: workload optimization, increasing engagement and the environment, the simulation of which reduced the predicted attrition by 22.4%."
  },
  {
    "objectID": "projects/comics-rec.html",
    "href": "projects/comics-rec.html",
    "title": "Comic book recommendation system",
    "section": "",
    "text": "This project used data from the Good Reads website, user ratings for comics, and comic characteristics.\nThe comic recommendation system is based on two approaches: collaborative filtering and content-based recommendations. The project included data preprocessing, sentiment analysis of descriptions, and network analysis.\nView the full report with code, graphs, and analysis"
  },
  {
    "objectID": "projects/comics-rec.html#project-summary",
    "href": "projects/comics-rec.html#project-summary",
    "title": "Comic book recommendation system",
    "section": "",
    "text": "This project used data from the Good Reads website, user ratings for comics, and comic characteristics.\nThe comic recommendation system is based on two approaches: collaborative filtering and content-based recommendations. The project included data preprocessing, sentiment analysis of descriptions, and network analysis.\nView the full report with code, graphs, and analysis"
  },
  {
    "objectID": "projects/comics-rec.html#methods-and-features",
    "href": "projects/comics-rec.html#methods-and-features",
    "title": "Comic book recommendation system",
    "section": "Methods and Features",
    "text": "Methods and Features\n\nCollaborative Filtering (UBCF): based on the user-item matrix and RMSE score.\nContent-based Recommendations: used features such as publisher, genres (shelves), and average sentiment.\nNetwork Analysis: visualization of relationships between comics (later abandoned as irrelevant for recommendations).\nResults evaluation: both RMSE/MAE and manual (using test users) validation of recommendations was performed (by comparing the characteristics of comics liked and recommended to the user)."
  },
  {
    "objectID": "projects/comics-rec.html#key-features",
    "href": "projects/comics-rec.html#key-features",
    "title": "Comic book recommendation system",
    "section": "Key Features",
    "text": "Key Features\n\nPreprocessing: adding test users and ratings, cleaning the data\nSentiment analysis: descriptions were broken down into words, and the average sentiment for each comic was calculated\nContent-based: a similarity matrix between comics was constructed based on the selected features"
  },
  {
    "objectID": "projects/comics-rec.html#conclusions",
    "href": "projects/comics-rec.html#conclusions",
    "title": "Comic book recommendation system",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe results of the network analysis showed that it was not useful for recommendation, so it was decided not to use it.\nA formal evaluation of collaborative filtering showed that the model built using the UBCF method performed better than the IBCF, as it had lower RMSE, MSE, and MAE values. Therefore, we used it to create the function. - By comparing the characteristics (validation) of comics that users liked and those recommended by the content-based system, we concluded that the content-based system worked well.\nBased on internal user ratings, the content-based system performed better in recommending comics than the collaborative filtering: the publisher names and user-generated shelves were the same as those that the user rated highly."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Comic book recommendation system\n\n\nCollaborative filtering (ALS, user-item matrix, RMSE estimation).\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nR&D Employee Churn Analysis\n\n\nSQL + R analysis: table joins, visualization, and logistic regression. Key churn factors (overtime, business trips) identified, HR dashboard.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nFE Models & Double ML (Master’s Thesis)\n\n\nFixed effects and double ML. Causal ML, heterogeneity analysis.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nKickstarter Data Analysis\n\n\nPredicting Project Success\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nAnalysis of Airline Data\n\n\nA Study of Lounge and Aircraft Ratings.\n\n\nRead more →"
  },
  {
    "objectID": "portfolio.html#machine-learning-and-modelling",
    "href": "portfolio.html#machine-learning-and-modelling",
    "title": "Portfolio",
    "section": "",
    "text": "Comic book recommendation system\n\n\nCollaborative filtering (ALS, user-item matrix, RMSE estimation).\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nR&D Employee Churn Analysis\n\n\nSQL + R analysis: table joins, visualization, and logistic regression. Key churn factors (overtime, business trips) identified, HR dashboard.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nFE Models & Double ML (Master’s Thesis)\n\n\nFixed effects and double ML. Causal ML, heterogeneity analysis.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nKickstarter Data Analysis\n\n\nPredicting Project Success\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nAnalysis of Airline Data\n\n\nA Study of Lounge and Aircraft Ratings.\n\n\nRead more →"
  },
  {
    "objectID": "portfolio.html#dashboards",
    "href": "portfolio.html#dashboards",
    "title": "Portfolio",
    "section": "Dashboards",
    "text": "Dashboards\n\n\n\n\n\n\nInteractive Dashboard: R&D Employee Attrition\n\n\nInteractive dashboard, created with Shiny.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nDeal Length Analysis: Dashboard\n\n\nDeal Length Dashboard: Calculates averages, displays line and bar charts. Data anonymized to preserve privacy. Created in Zoho Analytics.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nKickstarter Dashboard\n\n\nVisualization of project success by category, launch time, and name.\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nDashboard: Air Travel\n\n\nA study of lounge and aircraft ratings.\n\n\nRead more →"
  },
  {
    "objectID": "portfolio.html#bots-and-automation",
    "href": "portfolio.html#bots-and-automation",
    "title": "Portfolio",
    "section": "Bots and automation",
    "text": "Bots and automation\n\n\n\n\n\n\nTelegram bot: board game recommendations\n\n\nBot in R with inline mode, game database, filtering by genre\n\n\nRead more →\n\n\n\n\n\n\n\n\n\nTelegram bot: daily fitness based on HRV\n\n\nFitbit API integration, training fitness calculation, Telegram notifications.\n\n\nRead more →"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "If you’d like to get in touch, feel free to reach out via the platforms below:\n\n\n\n\n\n\n\nGitHub\nLinkedIn"
  },
  {
    "objectID": "contact.html#lets-connect",
    "href": "contact.html#lets-connect",
    "title": "Contact",
    "section": "",
    "text": "If you’d like to get in touch, feel free to reach out via the platforms below:\n\n\n\n\n\n\n\nGitHub\nLinkedIn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anna Pirogova",
    "section": "",
    "text": "I’ve been working with data for over 4 years: analyzing, visualizing, and helping businesses make data-driven decisions. I use Python, R, and SQL as my primary tools.\n\nEducation\nIstanbul Technical University (ITÜ)| Türkiye | QS 326 Master’s in Economics | 2023–2025 | GPA: 3.75/4.0\nBoğaziçi University (Bogaziçi)| Türkiye | QS 371 Exchange in Business Administration | 2022–2023 | GPA: 3.42/4.0\nHSE University, St. Petersburg| Russia Bachelor’s in International Business and Management | 2019–2023 | GPA 8.86/10 Minor: Data Science | 2019-2022 Diploma with honors | Average GPA: 8.86/10\n\n\nWork Experience\nPASV (EdTech startup, California, USA / remote) Data & CRM Analytics Specialist | May 2021 – present\nHSE University, St. Petersburg, Russia Teaching Assistant | September 2020 – June 2021\n\n\nKey Tools\nPython & R · SQL · Power BI · Streamlit · Pandas · CRM (Zoho) · Docker · Git\nGo to portfolio Download full CV Contact me:"
  },
  {
    "objectID": "reports/kickstarter-code.html",
    "href": "reports/kickstarter-code.html",
    "title": "Kickstarter - Code",
    "section": "",
    "text": "Data and libraries\n\nlibrary(readr)\nlibrary(lubridate)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(chron)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(coin)\nkickstarter = read_csv(\"~/documents/kickstarter.csv\")\n\nData transformation\n\nkickstarter$category=as.factor(kickstarter$category)\nkickstarter$main_category=as.factor(kickstarter$main_category)\nkickstarter$currency=as.factor(kickstarter$currency)\nkickstarter$state=as.factor(kickstarter$state)\nkickstarter$country=as.factor(kickstarter$country)\nkickstarter=kickstarter %&gt;% mutate(launch_date=as.Date(launched)) %&gt;% mutate(period_length=deadline-launch_date,month = month(launch_date),day = day(launch_date),launch_season = case_when(\n    month &gt; 2 & month &lt; 6 ~ \"spring\",\n    month &gt; 5 & month &lt; 9 ~ \"summer\",\n    month &gt; 8 & month &lt; 12 ~ \"autumn\",\n    month &gt; 11 | month &lt; 3 ~ \"winter\",\n    TRUE ~ \"unknown\"),launch_weekday=weekdays.Date(kickstarter$launch_date))\nkickstarter=kickstarter %&gt;% mutate(launch_weekday=weekdays.Date(kickstarter$launch_date),gen_category=str_trim(paste(main_category,\", \",category)),name_length=str_length(name),number_of_words=str_count(name,'\\\\w+'),n_caps_in_name=str_count(name, \"\\\\b[A-Z]{2,}\\\\b\"),has_caps_in_name=case_when(n_caps_in_name&gt;0 ~ 1, TRUE ~ 0),launch_hour=hour(launched), launch_dayPart = case_when(\n    launch_hour &gt; 5 & launch_hour &lt; 13 ~ \"morning\",\n    launch_hour &gt; 11 & launch_hour &lt; 18 ~ \"afternoon\",\n    launch_hour &gt; 17 & launch_hour &lt; 23 ~ \"evening\",\n    launch_hour &gt; 22 | launch_hour &lt; 6 ~ \"night\",\n    TRUE ~ \"unknown\"\n  ),launch_is_weekend=is.weekend(launched),cat_is_cat=case_when(as.character(main_category)==as.character(category)~1,TRUE~0),name_has_marks=as.factor(case_when(str_detect(name,\":\")==TRUE|str_detect(name,\";\")==TRUE|str_detect(name,\"!\")==TRUE~1,TRUE~0)))\n                                                                          \nkickstarter$launch_season=factor(kickstarter$launch_season,levels=c(\"autumn\",\"winter\",\"spring\",\"summer\"))\n\nkickstarter$launch_weekday=factor(kickstarter$launch_weekday,levels=c(\"Monday\",\"Tuesday\", \"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"))\n\nkickstarter$has_caps_in_name=as.factor(kickstarter$has_caps_in_name)\nkickstarter$cat_is_cat=as.factor(kickstarter$cat_is_cat)\nkickstarter$name_has_marks=as.factor(kickstarter$name_has_marks)\nkickstarter$gen_category=kickstarter$gen_category %&gt;% str_replace_all(\" ,  \",\", \")\n\nkickstarter=kickstarter %&gt;% mutate(gen_cat_length=str_length(gen_category),gen_cat_number_words=str_count(gen_category,'\\\\w+'),gen_cat_first_letter=str_extract(gen_category,'[A-Z]'),cat_length=str_length(category),cat_number_words=str_count(category,'\\\\w+'),cat_first_letter=str_extract(category,'[A-Z]'))\n\nkickstarter$gen_category=as.factor(kickstarter$gen_category)\nkickstarter$launch_dayPart=factor(kickstarter$launch_dayPart,levels=c(\"morning\",\"afternoon\",\"evening\",\"night\"))\nkickstarter$gen_cat_first_letter=as.factor(kickstarter$gen_cat_first_letter)\nkickstarter$cat_first_letter=as.factor(kickstarter$cat_first_letter)\n\nProjects by sum graph\n\nkickstarter_sum_by_category=kickstarter %&gt;% group_by(state,main_category) %&gt;% filter(state==\"successful\") %&gt;% summarise(average_sum=round(mean(usd_goal_real),1)) %&gt;% arrange(-average_sum) %&gt;% ungroup() %&gt;% dplyr::select(-state)\nknitr::kable(kickstarter_sum_by_category,col.names=c(\"Основная категория\",\"Средняя сумма реализованных проектов\"))\n\nSuccess of projects by category\n\nkickstarter_num_cat=kickstarter %&gt;% group_by(main_category) %&gt;% summarise(cnt=n())\nkickstarter_success_perc=kickstarter %&gt;% filter(state==\"successful\") %&gt;% group_by(main_category) %&gt;% summarise(s_cnt=n())\nkickstarter_success_by_cat=left_join(kickstarter_success_perc,kickstarter_num_cat,by=\"main_category\") %&gt;% mutate(success_percentage=s_cnt/cnt*100) %&gt;% arrange(-success_percentage) %&gt;% select(-s_cnt,-cnt)\nknitr::kable(kickstarter_success_by_cat,col.names=c(\"Основная категория\",\"Процент успешных проектов\"))\n\nGraph Success of projects by category\n\nggplot(data=kickstarter)+geom_bar(aes(x=main_category,fill=state),stat=\"count\")+xlab('Название основной категории')+ylab('Количество проектов')+ggtitle('Успешность проектов в разных категориях')+coord_flip()+scale_fill_brewer(name=\"Состояние проекта\",labels=c(\"неуспешный\",\"успешный\"),palette=\"Pastel1\")\n\nGraph Success of projects by Launch hour\n\nggplot(data=kickstarter)+geom_bar(aes(x=launch_hour,fill=state),position=\"fill\")+ylab(\"Доля проектов\")+xlab(\"Час запуска\")+scale_fill_brewer(name=\"Состояние проекта\",labels=c(\"неуспешный\",\"успешный\"),palette=\"Pastel1\")+ggtitle(\"Успешность проектов по часу запуска\")\n\nGraph Success of projects by title length\n\nggplot(data=kickstarter)+geom_boxplot(aes(x=state,y=name_length, fill=state))+ylab(\"Длина названия проекта в символах\")+xlab(\"Состояние проекта\")+scale_x_discrete(labels=c(\"Неуспешный\", \"Успешный\"))+ggtitle(\"Успешность проекта по длине названия проекта\")+scale_fill_brewer(name=\"Состояние проекта\",labels=c(\"неуспешный\",\"успешный\"),palette=\"Pastel1\")\n\nGraph Success of projects by special symbols in the title\n\nggplot(data=kickstarter)+geom_bar(aes(x=name_has_marks,fill=state),position=\"fill\")+xlab(\"Наличие символов :,!,; в названии проекта\")+ylab(\"Доля проектов\")+ggtitle(\"Успешность проекта по наличию символов в названии\")+\nscale_fill_brewer(name=\"Состояние проекта\",labels=c(\"неуспешный\",\"успешный\"),palette=\"Pastel1\")+scale_x_discrete(labels=c(\"Нет\", \"Да\"))\n\nModel creation\n\n# \"seed\" for the random number generator\nset.seed(1234) \n\n# Let's take 80% as training\nkickstarter_train = kickstarter %&gt;% dplyr::sample_frac(.8)\n\n# create a test dataset\n# using an anti-join to remove all observations that were in the training set\nkickstarter_test = dplyr::anti_join(kickstarter, kickstarter_train, by = 'id') %&gt;% dplyr::select(-id)\n\nkickstarter_train = kickstarter_train %&gt;% dplyr::select(-id)\n\n## Building a Complex Model\n## Let's try building a complex model that will give us the highest accuracy. We'll use the rpart package and all variables except id, name, date, deadline, launched, backers, and launch date.\n\n### Unpruned Tree\ntree1 &lt;- rpart(state ~ category+gen_category+main_category+usd_goal_real+day+country+month+launch_season+currency+period_length+launch_weekday+name_length+number_of_words+has_caps_in_name+n_caps_in_name+launch_dayPart+launch_hour+launch_is_weekend+cat_is_cat+name_has_marks+cat_length+cat_number_words+cat_first_letter+gen_cat_length+gen_cat_number_words+gen_cat_first_letter, method= \"class\",data = kickstarter_train, control = rpart.control(cp = 0.001)) \nrpart.plot::rpart.plot(tree1)\n\n#predict\npred.kick_train1 = predict(tree1, type=\"class\")\n\n## **accuracy on the training set**\nt1 = table(pred.kick_train1, kickstarter_train$state)\nacc1tr=(t1[1,1] + t1[2,2])/sum(t1)\nacc1tr\n\n## **accuracy on the test set**\npred.kick_test1 = predict(tree1, newdata = kickstarter_test, type=\"class\")\nt11 = table(pred.kick_test1, kickstarter_test$state)\nacc1ts=(t11[1,1] + t11[2,2])/sum(t11)\nacc1ts\n\n## As we can see, the accuracy on the test sample was lower, but only slightly. Let's prune the tree based on the optimal cp value.\n\n## **Building the cp table**\nset.seed(1234)\ncp_tbl = printcp(tree1)\nset.seed(1234)\nmin_cp = cp_tbl %&gt;%\n  as.data.frame() %&gt;% \n  arrange(xerror) %&gt;% \n  head(n = 1)\nmin_cp\n\n## **prune the tree using prune()**\ntree2 = prune(tree1, cp = 0.00102)\n\nrpart.plot::rpart.plot(tree2,fallen.leaves = FALSE,faclen=2)\n# making a prediction\npred.kick_train2=predict(tree2, type=\"class\")\n\n## **accuracy for training sample**\nt2 = table(pred.kick_train2, kickstarter_train$state)\nacc2tr=(t2[1,1] + t2[2,2])/sum(t2)\nacc2tr\n\n## **accuracy for the test sample**\n#prediction\npred.kick_test2 = predict(tree2, kickstarter_test, type=\"class\")\n#accuracy\nt22 = table(pred.kick_test2, kickstarter_test$state)\nacc2ts=(t22[1,1] + t22[2,2])/sum(t22)\nacc2ts\n## So, we've built a complex model and achieved an accuracy of 0.7018865 on the training set and 0.6823584 on the test set. Let's try building a less complex model that still achieves similar accuracy.\n## To do this, use the summary command to see which variables Model 1 considered most important and build a model based on them.\n\n## Less complicated model creation\n\n## **model and accuracy for the training set**\n\n\n#summary(tree2)\ntree0 &lt;- rpart(state ~ gen_category+category+main_category+usd_goal_real+number_of_words+cat_is_cat+launch_dayPart+period_length+cat_length+cat_number_words+cat_first_letter+gen_cat_length+gen_cat_number_words+gen_cat_first_letter+country+currency+launch_weekday+launch_hour, method=\"class\",data=kickstarter_train)\npred0 = predict(tree0, type=\"class\")\n#function for line breaks for variable names\nsplit.fun &lt;- function(x, labs, digits, varlen, faclen)\n{\n    # replace commas with spaces (needed for strwrap)\n    labs &lt;- gsub(\",\", \" \", labs)\n    for(i in 1:length(labs)) {\n        # split labs[i] into multiple lines\n        labs[i] &lt;- paste(strwrap(labs[i], width=32), collapse=\"\\n\")\n    }\n    labs\n}\nprp(tree0, split.fun=split.fun,fallen.leaves=FALSE,extra=4,cex=0.45)\n\nt0 = table(pred0, kickstarter_train$state)\n\n(t0[1,1] + t0[2,2])/sum(t0)\n\n## **accuracy for test sample**\npred00 = predict(tree0, kickstarter_test, type=\"class\")\n\nt = table(pred00, kickstarter_test$state)\n\n(t[1,1] + t[2,2])/sum(t)\n\nИтак, мы построили намного более простую модель, имеющую точность 0.6758971 для тренировочной и 0.6693204 для тестовой выборок.\n\na=(0.7018865-0.6758971)*100\nb=(0.6823584-0.6693204)*100\npaste(\"At the same time, the accuracy of the simplified model is \",round(a,2),\"% (training sample) and \",round(b,2),\"% (test sample) lower than that of an order of magnitude more complex model\")\n\n[1] \"At the same time, the accuracy of the simplified model is  2.6 % (training sample) and  1.3 % (test sample) lower than that of an order of magnitude more complex model\"\n\n\nImportance of variables in the model\n\ntreecat = c('category','gen_category','main_category','cat_first_letter','gen_cat_first_letter','cat_length','usd_goal_real','period_length','name_length','country','launch_dayPart','number_of_words','currency','launch_hour')\nmeaning = c('category', 'project category and subcategory', 'main category', 'first letter of category name', 'first letter of subcategory name', 'category name length in characters', 'project target amount in dollars', 'fundraising duration', 'project name length in characters', 'country', 'part of the day the project was launched', 'number of words in the project name', 'currency', 'launch hour')\nimportance=c('25','22','12','11','10','7','4','3','1','1','1','1','1','1')\ncatable = data.frame(treecat, meaning,importance)\nknitr::kable(catable, col.names = c(\"Variable\",\"Name Explanation\",\"Significance\"))\n\n\n\n\n\n\n\n\n\nVariable\nName Explanation\nSignificance\n\n\n\n\ncategory\ncategory\n25\n\n\ngen_category\nproject category and subcategory\n22\n\n\nmain_category\nmain category\n12\n\n\ncat_first_letter\nfirst letter of category name\n11\n\n\ngen_cat_first_letter\nfirst letter of subcategory name\n10\n\n\ncat_length\ncategory name length in characters\n7\n\n\nusd_goal_real\nproject target amount in dollars\n4\n\n\nperiod_length\nfundraising duration\n3\n\n\nname_length\nproject name length in characters\n1\n\n\ncountry\ncountry\n1\n\n\nlaunch_dayPart\npart of the day the project was launched\n1\n\n\nnumber_of_words\nnumber of words in the project name\n1\n\n\ncurrency\ncurrency\n1\n\n\nlaunch_hour\nlaunch hour\n1"
  }
]